# -*- coding: utf-8 -*-
"""modelogused_car_price prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lW5yGQSI24Cs6WxTaqQI-mZU9CfPRB2e
"""

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv("/content/Used Car Dataset.csv")

df.dtypes

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
# Convert to numeric format (year)
df['registration_year'] = pd.to_datetime(df['registration_year'], errors='coerce').dt.year
df['manufacturing_year'] = pd.to_datetime(df['manufacturing_year'], errors='coerce').dt.year

# One-Hot Encoding for 'insurance_validity', 'fuel_type', 'transmission', and 'ownership'
# Corrected list of columns for One-Hot Encoding
# Corrected list of columns for One-Hot Encoding
one_hot_columns = ['insurance_validity', 'fuel_type', 'transmission', 'ownsership']


one_hot_encoder = OneHotEncoder()
one_hot_encoded = one_hot_encoder.fit_transform(df[one_hot_columns])

# Create a DataFrame from the encoded columns
one_hot_df = pd.DataFrame(one_hot_encoded.toarray(),
                          columns=one_hot_encoder.get_feature_names_out(one_hot_columns))

# Merge the new DataFrame with the original and drop the original columns
df = df.join(one_hot_df).drop(columns=one_hot_columns)

# Frequency Encoding for 'car_name'
frequency_encode = df['car_name'].value_counts(normalize=True)
df['car_name'] = df['car_name'].map(frequency_encode)

df.head()

df.shape

from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

df = df.dropna()
X = df.drop(['max_power(bhp)', 'Unnamed: 0'], axis=1)
y = df['max_power(bhp)']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'XGBoost': XGBRegressor(),
    'Decision Tree': DecisionTreeRegressor()
}

# Loop to test each model
for model_name, model in models.items():
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_test)  # Make predictions on the test set

    # Calculate evaluation metrics
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)

    # Print the results for each model
    print(f'Model: {model_name}')
    print(f'R2 Score: {r2:.2f}')
    print(f'MAE: {mae:.2f}')

import matplotlib.pyplot as plt
import numpy as np


# Scatter plot for Linear Regression predictions
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', label='Actual vs. Predicted')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', label='Perfect Prediction Line')
plt.title('Linear Regression: Actual vs. Predicted')
plt.xlabel('Actual max_power(bhp)')
plt.ylabel('Predicted max_power(bhp)')
plt.legend()
plt.show()

import pandas as pd

# Assuming you have already trained and obtained predictions (y_pred) from your Linear Regression model
# If not, you may need to fit the Linear Regression model and make predictions first

# Create a DataFrame for actual and predicted values
results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})

# Display the actual and predicted values
print("Actual and Predicted Values:")
print(results_df)

